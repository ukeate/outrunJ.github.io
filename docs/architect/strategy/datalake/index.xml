<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ukeate的笔记</title>
    <link>https://ukeate.com/docs/architect/strategy/datalake/</link>
    <description>Recent content on ukeate的笔记</description>
    <generator>Hugo</generator>
    <language>en</language>
    <atom:link href="https://ukeate.com/docs/architect/strategy/datalake/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://ukeate.com/docs/architect/strategy/datalake/ecology/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ukeate.com/docs/architect/strategy/datalake/ecology/</guid>
      <description>&lt;h1 id=&#34;hadoop体系&#34;&gt;&#xA;  Hadoop体系&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#hadoop%e4%bd%93%e7%b3%bb&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;pre&gt;&lt;code&gt;发行版&#xA;    Apache Hadoop&#xA;    CDH(Cloudera&#39;s Distribution Including Apache Hadoop)&#xA;    HDP(Hortonworks Data Platform)&#xA;    宜信&#xA;        D.Bus&#xA;            # 数据收集与计算&#xA;        UAVStack&#xA;            # AIOps, 智能运维&#xA;            UAV.Monitor&#xA;                # 监控&#xA;            UAV.APM&#xA;                # 性能管理&#xA;            UAV.ServiceGovern&#xA;                # 服务治理&#xA;            UAV.MSCP&#xA;                # 微服务计算&#xA;        Wormhole&#xA;            # SPaaS(Stream Processing as a Service)&#xA;        Gartner&#xA;            # ITOA，算法即运维&#xA;大数据 &#xA;    PB级数据&#xA;    4V&#xA;        volume(大量)&#xA;        velocity(高速)&#xA;        variety(多样)&#xA;        value(低价值密度)&#xA;    场景&#xA;        物流仓储: 精细化运营，命中率&#xA;        推荐&#xA;        保险: 风险预测&#xA;        金融: 用户特征&#xA;        房产: 精准投策、营销&#xA;        AI&#xA;    组织部门&#xA;        平台: 集群&#xA;            Hadoop、Flume、Kafka、HBase、Spark等搭建&#xA;            性能监控、调优&#xA;        数据仓库: 写SQL&#xA;            ETL, 数据清洗&#xA;            Hive, 数据分析、建模&#xA;        数据挖掘&#xA;            数据支持&#xA;            算法、推荐、用户画像&#xA;        报表&#xA;            JavaEE&#xA;hadoop&#xA;    Apache开源, 分布式系统基础架构&#xA;    面临问题&#xA;        硬盘&#xA;            1块: 10TB-14TB &#xA;            1PB: 102块硬盘&#xA;        算&#xA;            MySQL5.5: 300w-500w&#xA;            MySQL8: 1亿、1GB&#xA;    Doug Cutting&#xA;        GFS -&amp;gt; HDFS&#xA;            存储&#xA;        Map-Reduce -&amp;gt; MapReduce&#xA;            计算&#xA;        BigTable -&amp;gt; HBase&#xA;            表式存储&#xA;    发展&#xA;        2003-2004: Google公开部分GFS和MapReduce&#xA;        2005: Hadoop成为Apache Lucene子项目Nutch了一部分&#xA;        2006.3: MapReduce和NDFS(Nutch Distributed File System)纳入Hadoop&#xA;    发行版本&#xA;        Apache: 开源&#xA;        Cloudera: Doug Cutting, 一键部署, 资源占用大&#xA;        Hortonworks: 雅虎工程师，贡献Hadoop 80%代码, 一键部署&#xA;        阿里云&#xA;    特点&#xA;        高可靠性：多副本&#xA;        高扩展性&#xA;        高效性: 并行运行&#xA;        高容错性&#xA;    组成&#xA;        Hadoop1.x&#xA;            HDFS(存), MapReduce(算、资源调度), Common&#xA;        Hadoop2.x&#xA;            HDFS(存), MapReduce(算), Yarn(资源调度), Common&#xA;        Hadoop3.x&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h2 id=&#34;hdfs&#34;&gt;&#xA;  HDFS&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#hdfs&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;code&gt;# Hadoop Distributed File System, 一开始为Nutch搜索引擎开发&#xA;存储模型&#xA;&#x9;按字节切割,block存储,block多副本&#xA;&#x9;不支持修改(因为修改文件而非block, 且会引发规模修改)，可以追加&#xA;主从架构&#xA;&#x9;NameNode&#xA;&#x9;&#x9;树形目录&#xA;&#x9;&#x9;内存存储元数据，可持久化(EditLog事务日志, FsImage)&#xA;&#x9;&#x9;&#x9;NameNode启动时安全模式&#xA;&#x9;&#x9;&#x9;&#x9;SecondaryNameNode合并EditLog到新FsImage&#xA;&#x9;&#x9;&#x9;&#x9;DataNode上报block列表&#xA;&#x9;&#x9;存副本策略&#xA;&#x9;DataNode&#xA;&#x9;&#x9;本地文件形式存block, 存校验&#xA;&#x9;&#x9;与NameNode心跳，汇报block列表&#xA;&#x9;Client&#xA;&#x9;&#x9;交互元数据和block&#xA;API结构&#xA;&#x9;推荐节点数不过5000&#xA;&#x9;角色：一个进程&#xA;Block副本放置策略&#xA;&#x9;Pipeline&#xA;HA&#xA;&#x9;JournalNode&#xA;&#x9;&#x9;NameNode同步EditLog&#xA;&#x9;FailoverController&#xA;&#x9;&#x9;利用ZooKeeper&#xA;&#x9;&#x9;同主机下监控NameNode&#xA;&#x9;&#x9;验证对方主机主NN是否真的挂掉，调用对方降级为Standby&#xA;问题及方案&#xA;    单点故障&#xA;        多NameNode，主备(2.x只能1主1备, 3.x可以1主5备)&#xA;    压力大，内存受限&#xA;        联帮: Federation(元数据分片)&#xA;配置网络&#xA;    /etc/sysconfig/network-scripts/ifcfg-eth0&#xA;    /etc/sysconfig/network&#xA;        NETWORKING=YES&#xA;        HOSTNAME=node01    &#xA;    /etc/hosts&#xA;    /etc/selinux/config&#xA;        SELINUX=disabled&#xA;    /etc/ntp.conf&#xA;        server htp1.aliyun.com&#xA;    /etc/profile&#xA;        export JAVA_HOME=/usr/java/default&#xA;        export PATH=$PATH:$JAVA_HOME/bin&#xA;    service iptables stop &amp;amp; chkconfig iptables off&#xA;    service ntp start &amp;amp; chkconfig ntp on&#xA;    配ssh免密登录&#xA;部署配置&#xA;    mkdir /opt/bigdata&#xA;    /etc/profile&#xA;        export HADOOP_HOME=/opt/bigdata/hadoop-2.6.5&#xA;        export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin&#xA;    /etc/hadoop&#xA;        hadoop-env.sh&#xA;            export JAVA_HOME=/usr/java/default&#xA;        core-site.xml&#xA;            &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;hdfs://node01:9000&amp;lt;/value&amp;gt;&#xA;        hdfs-site.xml&#xA;            &amp;lt;name&amp;gt;fs.replication&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;&#xA;            &amp;lt;name&amp;gt;dfs.namenode.name.dir&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;/var/bigdata/hadoop/local/dfs/name&amp;lt;/value&amp;gt;&#xA;                # namenode元数据&#xA;            &amp;lt;name&amp;gt;dfs.datanode.data.dir&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;/var/bigdata/hadoop/local/dfs/data&amp;lt;/value&amp;gt;&#xA;            &amp;lt;name&amp;gt;dfs.namenode.secondary.http-address&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;node01:50090&amp;lt;/value&amp;gt;&#xA;            &amp;lt;name&amp;gt;dfs.namenode.checkpoint.dir&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;/var/bigdata/hadoop/local/dfs/secondary&amp;lt;/value&amp;gt;&#xA;        slaves&#xA;            node1&#xA;命令&#xA;    hdfs namenode -format&#xA;    start-dfs.sh&#xA;    访问页面 node01:50070 node01:50090&#xA;    hdfs dfs -mkdir -p /user/root&#xA;    hdfs dfs -D dfs.blocksize=1048576 -put a.txt /user/root&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h2 id=&#34;使用&#34;&gt;&#xA;  使用&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%bd%bf%e7%94%a8&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;code&gt;软件结构&#xA;    0        jdk, Hadoop                        NameNode, DFSZKFailoverController&#xA;    1        jdk, Hadoop                        NameNode, DFSZKFailoverController&#xA;    2        jdk, Hadoop                        ResourceManager&#xA;    3        jdk, Hadoop, Zookeeper        DataNode, NodeManager, JournalNode, QuorumPeerMain&#xA;    4        jdk, Hadoop, Zookeeper        DataNode, NodeManager, JournalNode, QuorumPeerMain&#xA;    5        jdk, Hadoop, Zookeeper        DataNode, NodeManager, JournalNode, QuorumPeerMain&#xA;Zookeeper&#xA;    配置conf/zoo.cfg&#xA;        tickTime=2000                        # 心跳间隔(ms)&#xA;        initLimit=10                        # 初始化时最多容忍心跳次数&#xA;        syncLimit=5                        # 同步失败最多容忍心跳次数&#xA;        dataDir=/usr/local/Zookeeper/data        # 运行时文件目录&#xA;        clientPort=2181                # 运行端口号&#xA;        server.1=主机名或ip:2888:3888        # 服务运行端口与选举端口&#xA;        server.2=主机名或ip:2888:3888&#xA;        server.3=主机名或ip:2888:3888&#xA;    命令&#xA;        ./bin/zkServer.sh start&#xA;        ./bin/zkServer.sh status&#xA;        jps                                        # 显示名为QuorumPeerMain&#xA;Hadoop&#xA;    Hadoop-env.sh&#xA;        export JAVA_HOME=&#xA;    core-site.xml&#xA;        &amp;lt;configuration&amp;gt;&#xA;            &amp;lt;property&amp;gt;&#xA;                &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;&#xA;                &amp;lt;value&amp;gt;HDFS://ns1&amp;lt;/value&amp;gt;&#xA;            &amp;lt;/property&amp;gt;&#xA;            &amp;lt;property&amp;gt;&#xA;                &amp;lt;name&amp;gt;Hadoop.tmp.dir&amp;lt;/name&amp;gt;&#xA;                &amp;lt;value&amp;gt;/usr/local/Hadoop-2.2.0/tmp&amp;lt;/value&amp;gt;&#xA;            &amp;lt;/property&amp;gt;&#xA;            &amp;lt;property&amp;gt;&#xA;                &amp;lt;name&amp;gt;ha.Zookeeper.quorum&amp;lt;/name&amp;gt;&#xA;                &amp;lt;value&amp;gt;192.168.56.13:2181, 192.168.56.14:2181, 192.168.56.15:2181&amp;lt;/value&amp;gt;&#xA;            &amp;lt;/property&amp;gt;&#xA;        &amp;lt;/configuration&amp;gt;&#xA;    HDFS-site.xml&#xA;        &amp;lt;property&amp;gt;&#xA;            &amp;lt;name&amp;gt;dfs.nameservices&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;ns1&amp;lt;/value&amp;gt;&#xA;        &amp;lt;/property&amp;gt;&#xA;        &amp;lt;property&amp;gt;&#xA;            &amp;lt;name&amp;gt;dfs.ha.namenodes.ns1&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;nn1,nn2&amp;lt;/value&amp;gt;&#xA;        &amp;lt;/property&amp;gt;&#xA;        &amp;lt;property&amp;gt;&#xA;            &amp;lt;name&amp;gt;dfs.namenode.rpc-address.ns1.nn1&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;192.168.56.10:9000&amp;lt;/value&amp;gt;&#xA;        &amp;lt;/property&amp;gt;&#xA;        &amp;lt;property&amp;gt;&#xA;            &amp;lt;name&amp;gt;dfs.namenode.http-address.ns1.nn1&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;192.168.56.10:50070&amp;lt;/value&amp;gt;&#xA;        &amp;lt;/property&amp;gt;&#xA;        &amp;lt;property&amp;gt;&#xA;            &amp;lt;name&amp;gt;dfs.namenode.rpc-address.ns1.nn2&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;192.168.56.11:9000&amp;lt;/value&amp;gt;&#xA;        &amp;lt;/property&amp;gt;&#xA;        &amp;lt;property&amp;gt;&#xA;            &amp;lt;name&amp;gt;dfs.namenode.http-address.ns1.nn2&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;192.168.56.11:50070&amp;lt;/value&amp;gt;&#xA;        &amp;lt;/property&amp;gt;&#xA;        &amp;lt;property&amp;gt;&#xA;            &amp;lt;name&amp;gt;dfs.namenode.shared.edits.dir&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;qjournal://192.168.56.13:8485;192.168.56.14:8485;192.168.56.15:8485&amp;lt;/value&amp;gt;&#xA;        &amp;lt;/property&amp;gt;&#xA;        &amp;lt;property&amp;gt;&#xA;            &amp;lt;name&amp;gt;dfs.journalnode.edits.dir&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;/usr/local/Hadoop-2.2.0/journal&amp;lt;/value&amp;gt;&#xA;        &amp;lt;/property&amp;gt;&#xA;        &amp;lt;property&amp;gt;&#xA;            &amp;lt;name&amp;gt;dfs.ha.automatic-failover.enabled&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;&#xA;        &amp;lt;/property&amp;gt;&#xA;        &amp;lt;property&amp;gt;&#xA;            &amp;lt;name&amp;gt;dfs.client.failover.proxy.provider.ns1&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;org.Apache.Hadoop.HDFS.server.namenode.ha.ConfiguredFailoverProxyProvider&amp;lt;/value&amp;gt;&#xA;        &amp;lt;/property&amp;gt;&#xA;        &amp;lt;property&amp;gt;&#xA;            &amp;lt;name&amp;gt;dfs.ha.fencing.methods&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;sshfence&amp;lt;/value&amp;gt;&#xA;        &amp;lt;/property&amp;gt;&#xA;        &amp;lt;property&amp;gt;&#xA;            &amp;lt;name&amp;gt;dfs.ha.fencing.ssh.private-key-files&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;/root/.ssh/id_rsa&amp;lt;/value&amp;gt;&#xA;        &amp;lt;/property&amp;gt;&#xA;    mapred-site.xml&#xA;        &amp;lt;property&amp;gt;&#xA;            &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;Yarn&amp;lt;/value&amp;gt;&#xA;        &amp;lt;/property&amp;gt;&#xA;    Yarn-site.xml&#xA;        &amp;lt;property&amp;gt;&#xA;            &amp;lt;name&amp;gt;Yarn.resourcemanager.hostname&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;192.168.56.12&amp;lt;/value&amp;gt;&#xA;        &amp;lt;/property&amp;gt;&#xA;        &amp;lt;property&amp;gt;&#xA;            &amp;lt;name&amp;gt;Yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;&#xA;            &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt;&#xA;        &amp;lt;/property&amp;gt;&#xA;    etc/Hadoop/slaves&#xA;        192.168.56.13&#xA;        192.168.56.14&#xA;        192.168.56.15&#xA;收尾&#xA;    ssh免登录(0到1,2,3,4,5)&#xA;        ssh-keygen -t rsa&#xA;        ssh-copy-id -i 192.168.56.11            # 这样就可以免登录访问192.168.56.11了&#xA;                                                ## ssh-copy-id -i localhost 免登录自己&#xA;    复制Hadoop2.2.0(从0到1,2,3,4,5)&#xA;    添加Hadoop_home到环境变量&#xA;        etc/profile&#xA;            export HADOOP_HOME=/usr/local/Hadoop-2.2.0&#xA;            export PATH=$PATH:$HADOOP_HOME/bin&#xA;启动&#xA;    0 上启动&#xA;        ./sbin/Hadoop-daemons.sh start journalnode&#xA;    0 上格式化namenode&#xA;        Hadoop namenode -format&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h2 id=&#34;hbase&#34;&gt;&#xA;  HBase&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#hbase&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;code&gt;介绍&#xA;&#x9;Hadoop Database, 实时分布式, bigtable列簇数据库, 非结构化，自动切分, 并发读写&#xA;&#x9;只能row key查询, master有单点问题&#xA;版本&#xA;&#x9;0.98&#xA;&#x9;1.x&#xA;&#x9;2.x&#xA;原理&#xA;&#x9;修改只追加记录，合并时删除&#xA;架构&#xA;&#x9;Client&#xA;&#x9;&#x9;提供接口，维护客户端缓存&#xA;&#x9;Zookeeper&#xA;&#x9;&#x9;只有一个活跃master&#xA;&#x9;&#x9;存Region寻址入口&#xA;&#x9;&#x9;实时监控region server在线信息，通知master&#xA;&#x9;&#x9;存schema、table元数据&#xA;&#x9;Master&#xA;&#x9;&#x9;为region server分配region&#x9;&#xA;&#x9;&#x9;region server负载均衡&#xA;&#x9;&#x9;失效region server重新分配region&#xA;&#x9;&#x9;管理table CRUD&#xA;&#x9;RegionServer&#xA;&#x9;&#x9;维护region&#xA;&#x9;&#x9;切分大region&#xA;&#x9;Region&#xA;&#x9;&#x9;表水平分region分配在多个region server, region增大时裂变&#xA;&#x9;HLog&#xA;&#x9;&#x9;写Store之前先写HLog, flush到HDFS, store写完后HDFS存储移到old，2天后删除&#x9;&#xA;&#x9;Store&#xA;&#x9;&#x9;region由多个store组成, 一个store对应一个CF&#xA;&#x9;&#x9;store先写入memstore, 到阈值后启动flashcache写入storefile&#xA;&#x9;&#x9;storefile增长到阈值，进行合并&#xA;&#x9;&#x9;&#x9;minor compaction&#xA;&#x9;&#x9;&#x9;major compaction，默认最多256M&#xA;&#x9;&#x9;region所有storefile达到阈值，region分割&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h2 id=&#34;spark&#34;&gt;&#xA;  Spark&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#spark&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;code&gt;介绍&#xA;&#x9;in memory, 准实时的批处理，生态好于Storm&#xA;&#x9;无事务&#xA;集群&#xA;&#x9;Master&#xA;&#x9;Worker&#xA;&#x9;Driver&#xA;&#x9;Executor&#x9;&#xA;组件&#xA;&#x9;Spark RDD(Resiliennt Distributed Datasets)&#xA;&#x9;Spark Core 批计算，取代MR&#xA;&#x9;&#x9;粗粒度资源申请，task自行分配启动快，executor不kill&#xA;&#x9;&#x9;内存计算&#xA;&#x9;&#x9;chain&#xA;&#x9;Spark Streamming 流计算，取代Storm&#xA;&#x9;&#x9;批计算无限缩小，实时性差&#xA;&#x9;&#x9;默认无状态&#xA;&#x9;&#x9;&#x9;用updateStateByKey保存上次计算结果，变成有状态&#xA;&#x9;&#x9;&#x9;借助Redis或ES存&#xA;&#x9;Spark SQL 数据处理&#xA;&#x9;Spark MlLib 机器学习&#xA;&#x9;Spark R 数据分析&#xA;使用&#xA;&#x9;val session = SparkSessionBase.createSparkSession()&#xA;&#x9;var sc = session.sparkContext&#xA;&#x9;var rdd = sc.makeRDD(List(1,2,3,4,5,6))&#xA;&#x9;val mapRDD = rdd.map(x -&amp;gt; {&#xA;&#x9;&#x9;x&#xA;&#x9;})&#xA;&#x9;val filterRDD = mapRDD.filter(x =&amp;gt; {&#xA;&#x9;&#x9;true&#xA;&#x9;})&#xA;&#x9;filterRDD.count&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h1 id=&#34;独立体系&#34;&gt;&#xA;  独立体系&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%8b%ac%e7%ab%8b%e4%bd%93%e7%b3%bb&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;flink&#34;&gt;&#xA;  Flink&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#flink&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;code&gt;特点&#xA;&#x9;高吞吐、低延迟、高性能&#xA;&#x9;支持事件时间(Event Time)&#xA;&#x9;擅长有状态的计算&#x9;&#xA;&#x9;&#x9;内存&#xA;&#x9;&#x9;磁盘&#xA;&#x9;&#x9;RocksDB&#xA;&#x9;灵活的窗口（Window）操作： time, count, session&#xA;&#x9;基于轻量级分布式快照（CheckPoint）实现容错，保证exactly-once&#xA;&#x9;基于JVM实现独立内存管理&#xA;&#x9;Save Points方便代码升级&#xA;批计算是流计算的特例&#xA;&#x9;unbound streams&#x9;&#x9;# 定义开始不定义结束，流计算&#xA;&#x9;bounded streams&#x9;&#x9;# 定义开始也定义结束，批计算&#xA;迟到数据问题&#xA;&#x9;温度窗口&#xA;&#x9;水位线(Watermark)&#xA;集群&#xA;&#x9;JobManager(JVM进程)&#xA;&#x9;TaskManager(JVM进程)&#xA;&#x9;&#x9;Task Slot&#x9;&#xA;&#x9;&#x9;&#x9;一组固定的资源，隔离内存，不隔离核&#xA;&#x9;&#x9;&#x9;一般与核数对应，核支持超线程时一个算两个&#xA;配置&#xA;&#x9;/etc&#xA;&#x9;&#x9;/flink-conf.yaml&#xA;&#x9;&#x9;/slaves&#xA;&#x9;&#x9;/masters&#xA;组件&#xA;&#x9;部署&#xA;&#x9;&#x9;Single JVM&#x9;&#x9;# 多线程模拟&#xA;&#x9;&#x9;Standalone&#xA;&#x9;&#x9;YARN&#x9;&#xA;&#x9;库&#xA;&#x9;&#x9;CEP&#x9;&#x9;&#x9;&#x9;# 复杂事件库&#xA;&#x9;&#x9;Table&#xA;&#x9;&#x9;FlinkML&#xA;&#x9;&#x9;Gelly&#xA;使用&#xA;&#x9;import org.apache.flink.streaming.api.scala._&#xA;&#xA;&#x9;val env = StreamExecutionEnvironment.getExecutionEnvironment&#xA;&#x9;val initStream:DataStream[String] = env.socketTextStream(&amp;quot;node01&amp;quot;, 8888)&#xA;&#x9;val wordStream = initStream.flatMap(_.split(&amp;quot; &amp;quot;))&#xA;&#x9;val pairStream = wordStream.map((_, 1))&#xA;&#x9;val keyByStream = pairStream.keyBy(0)&#xA;&#x9;val restStream = keyByStream.sum(1)&#xA;&#x9;restStream.print()&#xA;&#x9;env.execute(&amp;quot;job1&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
  </channel>
</rss>
